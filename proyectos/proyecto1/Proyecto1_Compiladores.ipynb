{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Flex a Pedal\n",
        "- Juliana Nieto Cárdenas\n",
        "- Santiago Tovar Mosquera"
      ],
      "metadata": {
        "id": "p5RYi-bXIunJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "Este proyecto es un prototipo de un clon de **Flex**, el generador rápido de analizadores léxicos.\n",
        "\n",
        "**Flex** funciona de la siguiente forma\n",
        "1. Se lee el archivo de entrada con los escáneres a generar (expresados como expresiones regulares) y sus tokens correspondientes\n",
        "2. Luego, genera como salida un archivo `lex.yy.c`\n",
        "3. Cuando ejecutamos una entrada sobre este archivo, este busca que expresiones regulares se encuentran y ejecuta el escáner correspondiente."
      ],
      "metadata": {
        "id": "2BrzAR8vaoAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "## ¿Cómo funciona este \"Flex a Pedal\"?\n",
        "A grandes rasgos el proceso se resume en:\n",
        "- Construir los escáneres de tokens a partir de `user_tokens.txt`\n",
        "- Buscar tokens en `user_input.txt`"
      ],
      "metadata": {
        "id": "ZWuQzb33cAeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construcción de escáneres\n",
        "#### El archivo `user_tokens`\n",
        "Este archivo es una lista en donde en cada línea se específica un token y su expresión regular correspondiente de la siguiente forma:\n",
        "`NOMBRE_TOKEN -> REGEX_TOKEN`\n",
        "**Expresiones Regulares de este prototipo**\n",
        "\n",
        "Para este prototipo, simplificamos las expresiones regulares. Los símbolos válidos son:\n",
        "- Los rangos `[inicio-fin]`\n",
        "- La estrella de Kleene `*`\n",
        "- La suma de Kleene `+`\n",
        "- Inicio `^`\n",
        "- Fin `$`\n",
        "\n",
        "La disyunción es de la forma: `TOKEN1|TOKEN2` donde `TOKEN1` y `TOKEN2` son tokens definidos anteriormente en el archivo `user_tokens`\n",
        "\n",
        "#### De `regex` a `DFA`\n",
        "A cada token le corresponde un escáner.\n",
        "Un escáner, en este prototipo, es un *Autómata Finito Determinista* **(DFA)**. Esto se obtiene al transformar la expresión regular de dicho token en un *Autómata Finito No Determinista* **(NFA)** y este **NFA** en un **DFA**"
      ],
      "metadata": {
        "id": "CsQDeJ7jp9Tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "```\n",
        "Código\n",
        "```\n",
        "Para pasar de `regex` a `NFA` hemos implementado una clase llamada `NFA`. La cual es inicializada con una expresión regular de la siguiente forma\n",
        "\n",
        "\n",
        "```\n",
        "NFA(regex)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kAGubaWbtHyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NFA:\n",
        "  def __init__(self,re):\n",
        "    self.re = re\n",
        "    self.states = set([1])\n",
        "    self.accept = 1\n",
        "    self.sigma = dict()\n",
        "    self.current_state=max(self.states)        \n",
        "    self.last_current_state=list() \n",
        "    self.estado_union=1\n",
        "    i = 0\n",
        "    boolean_union=False\n",
        "    using_token=False\n",
        "    tokens_indexes = dict()\n",
        "    # tokens anteriores\n",
        "    for token in tokens:\n",
        "      tokens_indexes = {**tokens_indexes,**self.find_index(token)}\n",
        "    \n",
        "    while(i < len(re)):\n",
        "      if(i in tokens_indexes.keys()):\n",
        "        using_token=True\n",
        "        self.last_current_state.append(self.current_state)\n",
        "        token_name_nfa = tokens_indexes[i][0]\n",
        "        if(boolean_union):\n",
        "          self.new_union(token_name_nfa)\n",
        "          boolean_union=False          \n",
        "        else:\n",
        "          self.concatenate(token_name_nfa)\n",
        "        \n",
        "        \n",
        "        i+= (len(token_name_nfa) - 1)\n",
        "        \n",
        "      elif(re[i] == \"^\"):\n",
        "        if(re[i+1] != \"[\"):\n",
        "          self.starts_with(re[i+1])\n",
        "          i+=1\n",
        "        #en el caso donde solo sea ^char\n",
        "        if(len(re) ==2):\n",
        "          pass\n",
        "          #self.accept.add(max(self.states))\n",
        "          # loop con todas las letras en el ultimo estado\n",
        "          #self.alphanum_loop(max(self.states))\n",
        "        # skip ^char\n",
        "        #self.accept.add(max(self.states))\n",
        "      elif(re[i] == \"\\\\\"):\n",
        "        if(re[i+1] == \"s\"):\n",
        "          self.has_space()\n",
        "      elif(re[i] ==  \"\\t\"):\n",
        "        self.has_tab()\n",
        "      elif(re[i] == \"[\"):\n",
        "        self.last_current_state.append(self.current_state)\n",
        "      elif(re[i] == \"]\"):\n",
        "        if(boolean_union):\n",
        "          self.add_to_sigma(self.estado_union,\"~\",self.current_state)\n",
        "          boolean_union=False\n",
        "        if(i+1<len(re) and (re[i+1] == \"*\" or re[i+1] == \"+\")):\n",
        "          pass\n",
        "        else:\n",
        "          self.last_current_state.pop()\n",
        "      elif(i<len(re)-2 and re[i+1]==\"-\"):\n",
        "        self.rango(re[i],re[i+2]) \n",
        "        i+=2 \n",
        "      elif(re[i] == \"+\"):\n",
        "        if(re[i-1] == \"]\" or using_token):\n",
        "          self.mas(self.last_current_state.pop(),self.current_state) \n",
        "          using_token=False \n",
        "        else:\n",
        "          self.mas(self.current_state-1,self.current_state)\n",
        "      elif(re[i] == \"*\"):\n",
        "        if(re[i-1] == \"]\" or using_token):\n",
        "          self.clin(self.last_current_state.pop(),self.current_state)\n",
        "          using_token=False\n",
        "        else:\n",
        "          self.clin(self.current_state-1,self.current_state)  \n",
        "      elif(re[i]==\"|\"):\n",
        "          boolean_union=True\n",
        "      else:\n",
        "        self.just_a_letter(re[i],self.current_state)\n",
        "      self.current_state=max(self.states)        \n",
        "      i+=1\n",
        "\n",
        "    self.accept=max(self.states)\n",
        "  @property\n",
        "  def next_state(self):\n",
        "    return self.current_state+1     \n",
        "  \n",
        "  @property\n",
        "  def alphabet(self):\n",
        "    nfa_alphabet = set()\n",
        "    for q, c in self.sigma:\n",
        "      nfa_alphabet.add(c)\n",
        "    nfa_alphabet.discard(\"~\")\n",
        "    return \"\".join(nfa_alphabet)\n",
        "\n",
        "  def add_state(self):\n",
        "    new_state = self.next_state\n",
        "    self.states.add(new_state)\n",
        "    self.current_state=new_state\n",
        "    return new_state\n",
        "  \n",
        "  def add_to_sigma(self, q1,i,q2):\n",
        "    # q1: initial state, i: input char, q2: final state\n",
        "    q1_states = self.sigma.get((q1,i),set())\n",
        "    q1_states.add(q2)\n",
        "    self.sigma[(q1,i)] = q1_states \n",
        "  \n",
        "  # función para encontrar los índices de tokens anteriores en la regex actual\n",
        "  def find_index(self, token):\n",
        "    regex = self.re\n",
        "    indexes = dict()\n",
        "    for i in range(len(regex)-len(token)+1):\n",
        "      if regex[i:i+len(token)] == token:\n",
        "        indexes[i] = (token,i+len(regex))\n",
        "    return indexes\n",
        "#Función si leemos un unión\n",
        "  def union(self,re):\n",
        "      start_state = self.last_current_state[-1]\n",
        "      self.estado_union=self.current_state\n",
        "      disyun_state1 = self.add_state()\n",
        "      self.add_to_sigma(start_state, re, disyun_state1)\n",
        "     \n",
        "  #Función si leemos un rango\n",
        "  def rango(self,a,b):\n",
        "    current_state=self.current_state\n",
        "    cont=1\n",
        "    if(type(a)==str):\n",
        "      longitud=ord(b)+2-ord(a)\n",
        "      self.states.add(current_state+longitud)\n",
        "      for i in range(ord(a),ord(b)+1):\n",
        "        self.add_to_sigma(current_state,chr(i),self.add_state())\n",
        "        self.add_to_sigma(current_state+cont,\"~\",current_state+longitud)\n",
        "        cont=cont+1\n",
        "    else:\n",
        "      longitud=b+2-a\n",
        "      self.states.add(current_state+longitud)\n",
        "      for i in range(a,b+1):\n",
        "        self.add_to_sigma(current_state,i,self.add_state())\n",
        "        self.add_to_sigma(current_state+cont,\"~\",current_state+longitud)\n",
        "        cont=cont+1\n",
        "    self.current_state=max(self.states)\n",
        "  \n",
        "  #Función si leemos un mas\n",
        "  def mas(self,q,p): \n",
        "    self.add_to_sigma(p,\"~\",q)\n",
        "\n",
        "  #Función si leemos una estrella\n",
        "  def clin(self,q,p): \n",
        "    self.add_to_sigma(p,\"~\",q)\n",
        "    self.add_to_sigma(q,\"~\",p)\n",
        "  \n",
        "  def starts_with(self,c):\n",
        "    self.add_to_sigma(1,c,self.add_state())\n",
        "\n",
        "  \n",
        "  def has_tab(self):\n",
        "    current_state = self.current_state\n",
        "    c = \" \"\n",
        "    for i in range(4):\n",
        "      self.add_to_sigma(current_state,c,self.add_state())\n",
        "  \n",
        "  def has_space(self):\n",
        "    current_state = self.current_state\n",
        "    c = \" \"\n",
        "    self.add_to_sigma(current_state,c,self.add_state())\n",
        "\n",
        "  def just_a_letter(self,c,q):\n",
        "    self.add_to_sigma(q,c,self.add_state())\n",
        "  \n",
        "  def alphanum_loop(self,state):\n",
        "    all_chars = string.printable\n",
        "    for c in all_chars:\n",
        "         \n",
        "      now = (state,c)  \n",
        "      next_states = self.sigma.get(now,set())\n",
        "      next_states.add(state)\n",
        "      self.sigma[now] = next_states \n",
        "  def concatenate(self, token_name):\n",
        "    index_nfa = tokens.index(token_name)\n",
        "    token_nfa = my_nfas[index_nfa]\n",
        "    n = self.current_state\n",
        "    for q,c in token_nfa.sigma:      \n",
        "      old_tuple = (q,c)\n",
        "      old_transition = token_nfa.sigma[old_tuple]      \n",
        "      new_tuple = (n+(q-1),c)      \n",
        "      new_transition = set()\n",
        "      for s in old_transition:\n",
        "        self.states.add(n+(s-1))\n",
        "        new_transition.add(n+(s-1))\n",
        "      self.states.add(n+(q-1))\n",
        "      self.sigma[new_tuple]=new_transition\n",
        "  def new_union(self, token_name):\n",
        "    index_nfa = tokens.index(token_name)\n",
        "    token_nfa = my_nfas[index_nfa]\n",
        "    n = self.current_state\n",
        "    \n",
        "    for q,c in token_nfa.sigma:\n",
        "      new_state = n+q-1\n",
        "      if q==1:\n",
        "        new_state=1\n",
        "      old_transition = token_nfa.sigma[(q,c)]      \n",
        "      new_tuple = (new_state,c)    \n",
        "      new_transition = set()\n",
        "      for s in old_transition:\n",
        "        self.states.add(n+(s-1))\n",
        "        new_transition.add(n+(s-1))\n",
        "      self.states.add(new_state)\n",
        "      self.sigma[new_tuple]=new_transition\n",
        "\n",
        "\n",
        "    # estado del segundo token en la union\n",
        "    m = max(self.states)\n",
        "    # ultimas transiciones epsilon\n",
        "    last_state = m+1\n",
        "    self.states.add(last_state)\n",
        "    self.add_to_sigma(n,\"~\",last_state)\n",
        "    self.add_to_sigma(m,\"~\",last_state)\n",
        "    "
      ],
      "metadata": {
        "id": "3PpDuw69-LHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para pasar de **NFA** a **DFA** implementamos una clase llamada `DFA` la cual recibe un objeto `NFA` en la inicialización\n",
        "\n",
        "\n",
        "```\n",
        "DFA(nfa)\n",
        "```\n",
        "Con la información del objeto `NFA` recibido, se construye un **DFA** utilizando el algoritmo de subconjuntos\n"
      ],
      "metadata": {
        "id": "jgaBI7SItqoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DFA:\n",
        "  def __init__(self,nfa):\n",
        "    self.nfa = nfa\n",
        "    self.alphabet=nfa.alphabet\n",
        "    self.accept = set()\n",
        "    self.sigmaDFA = dict()\n",
        "    self.contruccion_por_subconjuntos()\n",
        "\n",
        "  def e_clau(self,state):\n",
        "    nfa=self.nfa\n",
        "    now=(state,\"~\")\n",
        "    s=set([state])\n",
        "    if( now in nfa.sigma):\n",
        "      s=s.union(nfa.sigma[now])  \n",
        "      for n in nfa.sigma[now]:\n",
        "        if((n,\"~\") in nfa.sigma):\n",
        "          s=s.union(nfa.sigma[(n,\"~\")])\n",
        "          for m in nfa.sigma[(n,\"~\")]:\n",
        "            if((m,\"~\") in nfa.sigma):\n",
        "              s=s.union(nfa.sigma[(m,\"~\")])\n",
        "    return s  \n",
        "\n",
        "  def contruccion_por_subconjuntos(self):\n",
        "    nfa=self.nfa\n",
        "    d1=self.e_clau(1)\n",
        "    lista_aux=list([d1])\n",
        "    lista_d=list([d1])\n",
        "    while(len(lista_aux)):\n",
        "      d=lista_aux.pop()\n",
        "      all_chars=self.alphabet\n",
        "      for c in all_chars:\n",
        "         T=set()\n",
        "         for s in d:\n",
        "            if(s==nfa.accept):\n",
        "              self.accept.add(lista_d.index(d)+1)\n",
        "            if ((s,c) not in nfa.sigma):\n",
        "              pass\n",
        "            else:\n",
        "              T=nfa.sigma[(s,c)]\n",
        "              d1=set()\n",
        "              for n in T:\n",
        "                d1=d1.union(self.e_clau(n)) \n",
        "              if(d1 not in lista_d):    \n",
        "                lista_aux.append(d1)\n",
        "                lista_d.append(d1)\n",
        "              now=(lista_d.index(d)+1,c)\n",
        "              self.sigmaDFA[now]=lista_d.index(d1)+1    \n",
        "\n",
        "  def ex(self,re):\n",
        "    current_st=1\n",
        "    for c in re:\n",
        "      if((current_st,c) in self.sigmaDFA):\n",
        "        current_st=self.sigmaDFA[(current_st,c)]\n",
        "      else:\n",
        "        return False\n",
        "    if(current_st in self.accept):    \n",
        "      return True      \n",
        "    else:\n",
        "      print(f\"estado actual {current_st}\")\n",
        "      return False  "
      ],
      "metadata": {
        "id": "cokRgQ30FwZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "### Buscar tokens\n",
        "Creamos un objeto llamado `execution` que lleva el registro de estado de un **DFA** con cierta cadena de caracteres\n"
      ],
      "metadata": {
        "id": "xnYqKGBwuW1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class execution:\n",
        "  def __init__(self,token,dfa):\n",
        "    self.token = token\n",
        "    self.dfa = dfa\n",
        "    self.current_state = 1\n",
        "    self.s = \"\"\n",
        "\n",
        "  def reset(self):\n",
        "    self.current_state = 1\n",
        "    self.s = \"\"\n",
        "\n",
        "\n",
        "  def execute(self,character,character_index):\n",
        "\n",
        "    # si se esta empezando la ejecucion, guardar indice\n",
        "    if(self.s==\"\"):\n",
        "      self.start_index = character_index\n",
        "    \n",
        "    # si se puede mover el DFA con el character\n",
        "    if((self.current_state,character) in self.dfa.sigmaDFA):\n",
        "\n",
        "      self.current_state=self.dfa.sigmaDFA[(self.current_state,character)]\n",
        "      self.s += character\n",
        "\n",
        "      # si se acepta la cadena\n",
        "      if(self.current_state in self.dfa.accept):\n",
        "        self.end_index = character_index\n",
        "        return True\n",
        "      \n",
        "  def look(self,character):\n",
        "    return (self.current_state,character) in self.dfa.sigmaDFA\n",
        "    "
      ],
      "metadata": {
        "id": "PFpugehADJ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flex a pedal en acción\n"
      ],
      "metadata": {
        "id": "tyN8MtBRYGXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creación de escáneres\n",
        "Se crean los escáneres del archivo `user_tokens.txt` y se almacenan en la lista `automatas`"
      ],
      "metadata": {
        "id": "VLzkQ6c9ZehS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list()\n",
        "my_nfas = list()\n",
        "automatas = list()\n",
        "input_file = open('user_tokens.txt','r')\n",
        "file_lines = input_file.readlines()\n",
        "for line in file_lines:\n",
        "  line = line.strip(\"\\n\")\n",
        "  token, regex = line.split(\" -> \")\n",
        "  new_NFA = NFA(regex)\n",
        "  my_nfas.append(new_NFA)\n",
        "  new_DFA = DFA(new_NFA) \n",
        "  tokens.append(token)\n",
        "  automatas.append(new_DFA)\n",
        "\n"
      ],
      "metadata": {
        "id": "X_JSgryLZzJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graficación de autómatas"
      ],
      "metadata": {
        "id": "NS_1wqSaEEVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fctBYz-EEGak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_automata(my_automata):\n",
        "  # crea nodos con etiqueta\n",
        "  my_sigma = my_automata.sigmaDFA\n",
        "  my_edge_labels = dict()\n",
        "  for t in my_sigma:\n",
        "    state_1, label = t\n",
        "    state_2 = my_sigma[t]\n",
        "    my_edge_labels[(state_1,state_2)] = label\n",
        "  \n",
        "  # crea el multidigrafo con los nodos+etiqueta\n",
        "  G = nx.MultiDiGraph()\n",
        "  G.add_nodes_from(automatas[0].states)\n",
        "  for edge in my_edge_labels:\n",
        "    G.add_edge(*edge)\n",
        "\n",
        "  # mapa de color para identificar nodos con estados de aceptacion\n",
        "  color_map=list()\n",
        "  for state in automatas[0].states:\n",
        "    node_color=\"gray\"\n",
        "    if state in automatas[0].accept:\n",
        "      node_color=\"green\"\n",
        "    color_map.append(node_color)\n",
        "\n",
        "  # grafica el grafo\n",
        "  pos = nx.shell_layout(G)\n",
        "  nx.draw(G, pos,with_labels=True,node_color=color_map, font_weight='bold',node_size=1000)\n",
        "  nx.draw_networkx_edge_labels(\n",
        "      G, pos,\n",
        "      edge_labels=my_edge_labels\n",
        "  )\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "-yK36_jw5jSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for automata in automatas:\n",
        "  draw_automata(automata)"
      ],
      "metadata": {
        "id": "42cCaFcaDYfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reconocimiento de tokens\n",
        "Luego se reconocen los tokens en el archivo `user_input.txt`"
      ],
      "metadata": {
        "id": "g1rCuERYYsYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = open('user_input.txt','r')\n",
        "file_lines = input_file.readlines()\n",
        "exs = list()\n",
        "# elimina tokens auxiliares\n",
        "for i in range(len(tokens)):\n",
        "  if not tokens[i].isupper():\n",
        "    exs.append(execution(tokens[i],automatas[i]))\n",
        "\n",
        "for line in file_lines:\n",
        "  for i in range(len(line)):\n",
        "    for ex in exs:\n",
        "      if(ex.execute(line[i],i)):\n",
        "        if(i==len(line)-1 or not ex.look(line[i+1])):\n",
        "          print(f\"Tipo de token: {ex.token} - Posición inicial:{ex.start_index} - Posición final:{ex.end_index} - valor: ‘{ex.s}’\")\n",
        "          ex.reset()"
      ],
      "metadata": {
        "id": "e7xfkVaUaxSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte B\n",
        "1. - `KEY`: Solo cadenas de numeros y letras que empiezan y terminan por `\"`\n",
        "  - `VALUE`: Puede ser un numero o una cadena que empieza y termina por `\"`\n",
        "2.  - `KEY` -> `\"[[0-9]|[a-z]]+\"`\n",
        "  - `VALUE` -> `[0-9]+ | \"[a-z]+\"` \n",
        "  "
      ],
      "metadata": {
        "id": "SvJIWE8POT5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = list()\n",
        "my_nfas = list()\n",
        "automatas = list()\n",
        "input_file = open('json_tokens.txt','r')\n",
        "file_lines = input_file.readlines()\n",
        "for line in file_lines:\n",
        "  line = line.strip(\"\\n\")\n",
        "  token, regex = line.split(\" -> \")\n",
        "  new_NFA = NFA(regex)\n",
        "  my_nfas.append(new_NFA)\n",
        "  new_DFA = DFA(new_NFA) \n",
        "  tokens.append(token)\n",
        "  automatas.append(new_DFA)\n",
        "\n"
      ],
      "metadata": {
        "id": "TFQGfiX-c7Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_file = open('json_input.txt','r')\n",
        "file_lines = input_file.readlines()\n",
        "exs = list()\n",
        "# elimina tokens auxiliares\n",
        "for i in range(len(tokens)):\n",
        "  if not tokens[i].isupper():\n",
        "    exs.append(execution(tokens[i],automatas[i]))\n",
        "\n",
        "for line in file_lines:\n",
        "  for i in range(len(line)):\n",
        "    for ex in exs:\n",
        "      if(ex.execute(line[i],i)):\n",
        "        if(i==len(line)-1 or not ex.look(line[i+1])):\n",
        "          print(f\"Tipo de token: {ex.token} - Posición inicial:{ex.start_index} - Posición final:{ex.end_index} - valor: ‘{ex.s}’\")\n",
        "          ex.reset()"
      ],
      "metadata": {
        "id": "yydxZibibEkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte C\n",
        "- **Tokenización en subpalabras**: Es la estrategia híbrida entre tokenización con palabras y caracteres. Les asignamos un identificador a las palabras más frecuentes y las palabras raras las descomponemos a partir de palabras más frecuentes y las identificamos en función de estas. Por ejemplo: \n",
        "  - **casa** es una palabra frecuente, supongamos que la identificamos con el número 100,\n",
        "  - y para las palabras que representan plural como \"**s**\" le asignamos el número 200. \n",
        "  - Luego **casas**, la identificamos como 100 y 200\n",
        "\n",
        "- **Tokenización BPE:** Consiste en identificar que par de bits van frecuentemente unidos, y hacer un cambio de variable\n",
        "Por ejemplo: Tomemos las palabras claves de Python `int` e `in`. Podriamos reemplazar `in` por `x` y queda `xt` y `x`. \n",
        "- **Algoritmo Unigram:** Es un algoritmo de tokenización por subpalabras basado en el modelo de lenguaje, Unigrama. Consiste en \n",
        "\n",
        "  1) Asignar a cada símbolo en el vocabulario inicial una probabilidad. La cual es:\n",
        "  $\\frac{\\text{número de veces que aparece en el texto}}{\\text{total de palabras en el texto}}$\n",
        "\n",
        "  2) Luego, se calcula la pérdida de quitar un símbolo del vocabulario\n",
        "  3) Se quitan los símbolos cuya pérdida es la menor\n",
        "\n",
        "- Los parsers **basados en el lenguaje natural**, nos permiten extraer un significado más comprensible al humano a diferencia de las anteriores estrategias vistas. Pero un problema que sí tienen estos tipos de parsers es lidiar con la ambigüedad del lenguaje humano."
      ],
      "metadata": {
        "id": "D7jwQkFc85cH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DY-YV79W8jlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}